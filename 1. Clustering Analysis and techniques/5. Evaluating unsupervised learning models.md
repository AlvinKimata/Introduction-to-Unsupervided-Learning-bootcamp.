## How evaluation here is different from supervised learning.
In case of supervised learning, evaluation is mostly done by measuring the performance metrics such as accuracy, precision, recall, AUC, etc. on the training set and the holdout sets.

However, in case of unsupervised learning, the process is not very straight forward as we do not have the ground truth (the labels). In the absence of labels, it is very diﬃcult to identify how results can be validated.

### Existing domain knowledge.
Let’s say we have a problem at hand to **cluster different songs in Spotify together on the basis of genres, to create different playlists**. After our work is done, how do we know it is good enough?

We can verify the results of our clustering exercise through our existing knowledge of the data (for example, knowing that genre A and genre B of music are similar so if those clusters are located near, it should be correct.


## Evaluation techniques.
There are two classes of statistical techniques used to validate results for cluster learning.

### 1. External validation.
This type of result validation can be carried out if true cluster labels are available.

In this approach we will have a set of clusters
_S= {C1, C2, C3,…………, Cn }_ which have been generated as a result of some clustering algorithm. 

We will have another set of clusters _P = {D1, D2, D3, …………, Dm}_ which represent the true cluster labels on the same data. 

The idea is to measure the **statistical similarity** between the two sets. A cluster set is considered as good if it is highly similar to the true cluster set.

In order to measure the similarity between S and P, we label each pair of records from the data as **positive** if the pairs belong to the **same cluster** in both P and S, else **negative**.

We then compute a `confusion matrix` between pair labels of S and P which can be used to measure the similarity.

- True Positive: The number of pairs of records which are in the same cluster, for both S and P.

- False Positive: The number of pairs of records which are in the same cluster in S but not P.

- False Negative: Number of pairs of records which are in the same cluster P but not in S.

- True Negative: Number of pairs of records which are not in the same cluser S as well as P.

## Matrix Representation.

We can represent our results in a matrix, showing what percentage of each playlist’s songs have ended up in each cluster.

### Matrix representation for `Ward Linkage`
![ward_linkage](inputs/ward_linkage.png)

Ward linkage tries to minimize variance within clusters,though there is some leakage into cluster B i.e in the 2nd column, there are entries in multiple clusters and not just one.


### Matrix representation for `Complete Linkage`
![complete_linkage](inputs/complete_linkage.png)

Complete linkage has not worked well since it has placed a lot of the dataset into cluster A. Cluster C consists of one single rap song.

### Matrix representation for `Average Linkage`
![average_linkage](inputs/average_linkage.png)

Many data points have been placed into a single cluster, with two clusters consisting of a single song.

### Matrix representation for `K-Means`
![k-means](inputs/K-Means.png)

K-Means clustering has done a good job across most of the algorithms, with some jazz and rap songs being ‘mistaken’ for K-Pop.


## Metrics used to assign cluster quality.

### Adjusted Rand Index.
The `Adjusted Rand Index` is used to express what prportion of the cluster assignments are `correct`. It computes a similarity measure between two different clusterings by considering all pairs of samples, and counting pairs that are assigned in the same or different clusters predicted, against the true clusterings, adjusting for random chance.

```python
from sklearn import metrics

labels_true = [0, 0, 1, 1, 1, 1]
labels_pred = [0, 0, 2, 2, 3, 3]

metrics.adjusted_rand_score(labels_true, labels_pred)
```
#### Output

```text
     0.4444444444444445
```

The Adjusted Rand index is bounded between `-1 and 1`. Closer to 1 is **good**, while closer to -1 is **bad**.


### Fowlkes-Mallows Score.
The Fowlkes-Mallows function measures the similarity of two clustering of a set of points. It may be defined as the geometric mean of the pairwise precision and recall.

```python
from sklearn.metrics.cluster import fowlkes_mellows_score

labels_true = [0, 0, 1, 1, 1, 1]
labels_pred = [0, 0, 2, 2, 3, 3]

fowlkes_mellows_score(labels_true, labels_pred)

```
#### Output

```text
     0.6546536707079771
```

Other external validation techniques include Jaccard Similarity and Mutual Information.

## Internal validation.
It involves using metrics where original labels are not required to evaluate clusters.

Metrics used for internal validation include:
- Cohesion within each cluster.
- Separation between different clusters.

### Internal Validation Metrics
Instead of dealing with two metrics, several measures are available which combine cohesion and coupling into a single measure. 

Few examples of such measures are:
- Silhouette coefficient
- Calisnki-Harabasz coefficient
- Dunn index
- Xie-Beni score
- Hartigan index
